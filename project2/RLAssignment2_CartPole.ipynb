{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "RLAssignment2_CartPole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4oDQxiZWA2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSytWBZBWAy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcKt5p00WAsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpwLbhorVCwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKf62-jBefMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHUMu0JVCwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import time\n",
        "import imageio\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('CartPole-v1').unwrapped\n",
        "\n",
        "# set up matplotlib\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython:\n",
        "    from IPython import display\n",
        "\n",
        "plt.ion()\n",
        "\n",
        "# if gpu is to be used\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GhGREIxb8is",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynsWmTacIBLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, size, frame_height=84, frame_width=84, channels=3, batch_size=32):\n",
        "        self.size = size\n",
        "        \n",
        "        self.frame_height = frame_height\n",
        "        self.frame_width = frame_width\n",
        "        self.channels = channels\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "\n",
        "        self.rewards = torch.empty(self.size, device=device, dtype=torch.uint8)\n",
        "        self.actions = torch.empty(self.size, device=device, dtype=torch.uint8)\n",
        "        self.states = torch.empty((self.size, self.channels, self.frame_height, self.frame_width), device=device,dtype=torch.uint8)\n",
        "        self.next_states = torch.empty((self.size, self.channels, self.frame_height, self.frame_width), device=device,dtype=torch.uint8)\n",
        "        self.done_flags = torch.empty(self.size, device=device, dtype=torch.uint8)\n",
        "\n",
        "        self.batch_states = torch.empty((self.batch_size, self.channels, self.frame_height, self.frame_width), device=device, dtype=torch.uint8)\n",
        "        self.batch_next_states = torch.empty((self.batch_size, self.channels, self.frame_height, self.frame_width), device=device, dtype=torch.uint8)\n",
        "        self.indices = torch.empty(self.batch_size, dtype=torch.int16)\n",
        "\n",
        "        self.current_position = 0\n",
        "        self.current_size = 0\n",
        "\n",
        "    def add_experience(self, state, action, next_state, reward, done):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if state.shape != (self.channels, self.frame_height, self.frame_width):\n",
        "          print (state.shape)\n",
        "          raise ValueError(\"Frame dimension is of wrong size\")\n",
        "\n",
        "        self.actions[self.current_position] = action\n",
        "        self.rewards[self.current_position] = reward\n",
        "        self.done_flags[self.current_position] = done\n",
        "        # import pdb; pdb.set_trace()\n",
        "        self.states[self.current_position, ...] = 255*state\n",
        "        self.next_states[self.current_position, ...] = 255*next_state\n",
        "\n",
        "        self.current_size = max(self.current_size, self.current_position)\n",
        "        self.current_position = (self.current_position + 1) % self.size\n",
        "\n",
        "    def get_MiniBatch(self):\n",
        "        if self.current_size < self.batch_size:\n",
        "            raise ValueError(\"Not enough experience in memory\")\n",
        "        \n",
        "        for i in range(self.batch_size):\n",
        "            self.indices[i] = random.randint(0, self.current_size-1)\n",
        "\n",
        "        # import pdb; pdb.set_trace();\n",
        "        for i, idx in enumerate(self.indices):\n",
        "            self.batch_states[i] = self.states[idx]\n",
        "            self.batch_next_states[i] = self.next_states[idx+1]\n",
        "    #   plt.imshow(np.array(self.states[0,0,:,:].cpu()))\n",
        "        self.indices = self.indices.type(torch.LongTensor)\n",
        "\n",
        "        return self.batch_states.type(torch.FloatTensor).to(device), self.actions[self.indices].unsqueeze(1).type(torch.LongTensor).to(device), self.rewards[self.indices].to(device), \\\n",
        "        self.batch_next_states.type(torch.FloatTensor).to(device), self.done_flags[self.indices].to(device)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.current_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIPoCxV9VCwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.head = nn.Linear(448, env.action_space.n)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        return self.head(x.view(x.size(0), -1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wz--_4HXVCws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resize_screen = T.Compose([T.ToPILImage(),\n",
        "                    T.Resize(40, interpolation=Image.CUBIC),\n",
        "                    T.ToTensor()])\n",
        "\n",
        "# This is based on the code from gym.\n",
        "screen_width = 600\n",
        "\n",
        "\n",
        "def get_cart_location():\n",
        "    world_width = env.x_threshold * 2\n",
        "    scale = screen_width / world_width\n",
        "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
        "\n",
        "\n",
        "def get_screen():\n",
        "    screen = env.render(mode='rgb_array').transpose(\n",
        "        (2, 0, 1))  # transpose into torch order (CHW)\n",
        "    # Strip off the top and bottom of the screen\n",
        "    screen = screen[:, 160:320]\n",
        "    view_width = 320\n",
        "    cart_location = get_cart_location()\n",
        "    if cart_location < view_width // 2:\n",
        "        slice_range = slice(view_width)\n",
        "    elif cart_location > (screen_width - view_width // 2):\n",
        "        slice_range = slice(-view_width, None)\n",
        "    else:\n",
        "        slice_range = slice(cart_location - view_width // 2,\n",
        "                            cart_location + view_width // 2)\n",
        "    # Strip off the edges, so that we have a square image centered on a cart\n",
        "    screen = screen[:, :, slice_range]\n",
        "    # Convert to float, rescare, convert to torch tensor\n",
        "    # (this doesn't require a copy)\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "    # Resize, and add a batch dimension (BCHW)\n",
        "    return resize_screen(screen).to(device)\n",
        "\n",
        "\n",
        "# print (get_screen().shape)\n",
        "env.reset()\n",
        "plt.figure()\n",
        "plt.imshow(get_screen().cpu().permute(1, 2, 0).numpy(),\n",
        "           interpolation='none')\n",
        "plt.title('Example extracted screen')\n",
        "plt.show()\n",
        "print (get_screen().shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9zHJE_4VCwv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "TARGET_UPDATE = 10\n",
        "\n",
        "policy_net = DQN(env).to(device)\n",
        "target_net = DQN(env).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "# target_net.eval()\n",
        "\n",
        "optimizer = optim.RMSprop(policy_net.parameters())\n",
        "replay_memory = ReplayMemory(10000, frame_height=40, frame_width=80, batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "\n",
        "def select_action(state, num_actions):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    steps_done += 1\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "            return policy_net(state.unsqueeze(0)).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[random.randrange(num_actions)]], device=device, dtype=torch.long)\n",
        "\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "def plot_durations(clear_screen=True):\n",
        "    plt.figure(2)\n",
        "    plt.clf()\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "    # Take 100 episode averages and plot them too\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "    if clear_screen:\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpbATD3xfIfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSc8XgUgMBqK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.transform import resize\n",
        "def generate_gif(frame_number, frames_for_gif, reward, path):\n",
        "    \"\"\"\n",
        "        Args:\n",
        "            frame_number: Integer, determining the number of the current frame\n",
        "            frames_for_gif: A sequence of (210, 160, 3) frames of an Atari game in RGB\n",
        "            reward: Integer, Total reward of the episode that es ouputted as a gif\n",
        "            path: String, path where gif is saved\n",
        "    \"\"\"\n",
        "    for idx, frame_idx in enumerate(frames_for_gif): \n",
        "        frames_for_gif[idx] = resize(frame_idx, (320, 320, 3), \n",
        "                                     preserve_range=True, order=0)\n",
        "        \n",
        "    imageio.mimsave(f'{path}{\"ATARI_CARTPOLE_DDQN_frame_{0}_reward_{1}.gif\".format(frame_number, reward)}', \n",
        "                    frames_for_gif, duration=1/30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ROEU1h5wLMe3",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    states, actions, rewards, new_states, terminal_flags = replay_memory.get_MiniBatch()\n",
        "\n",
        "    #DQN\n",
        "    # q_value = policy_net(states).gather(1, actions)\n",
        "    # next_q_values = target_net(new_states).max(1)[0].detach()\n",
        "    # expected_state_action_values = rewards + (GAMMA * next_q_values * (1 - terminal_flags)) \n",
        "    \n",
        "\n",
        "    q_values = policy_net(states)\n",
        "    next_q_values = policy_net(new_states)\n",
        "    next_q_state_values = target_net(new_states) \n",
        "\n",
        "    q_value = q_values.gather(1, actions)\n",
        "    next_q_value = next_q_state_values.gather(1, next_q_values.max(1)[1].unsqueeze(1))\n",
        "    expected_state_action_values = reward + GAMMA * next_q_value * (1 - terminal_flags)\n",
        "\n",
        "    loss = F.smooth_l1_loss(q_value, expected_state_action_values.unsqueeze(1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECBNXogrVCw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_episodes = 2001\n",
        "EVAL_FREQ = 200\n",
        "file_counter = 0\n",
        "num_actions = env.action_space.n\n",
        "\n",
        "all_rewards = []\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for i_episode in range(1, num_episodes):\n",
        "    # Initialize the environment and state\n",
        "    env.reset()\n",
        "    last_screen = get_screen()\n",
        "    current_screen = get_screen()\n",
        "    state = current_screen - last_screen\n",
        "    loss = []\n",
        "    episode_reward = 0\n",
        "    for t in count():\n",
        "        # Select and perform an action\n",
        "        action = select_action(state, num_actions)\n",
        "        _, reward, done, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "\n",
        "        # Observe new state\n",
        "        last_screen = current_screen\n",
        "        current_screen = get_screen()\n",
        "        # frames_for_gif.append(current_screen.cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
        "        # if not done:\n",
        "        next_state = current_screen - last_screen\n",
        "        # else:\n",
        "            # next_state = torch.FloatTensor([0])\n",
        "\n",
        "        # Store the transition in memory\n",
        "        replay_memory.add_experience(state=state, action=action, next_state=next_state, reward=reward, done=torch.FloatTensor([done]))\n",
        "\n",
        "        # Move to the next state\n",
        "        state = next_state\n",
        "\n",
        "        # Perform one step of the optimization (on the target network)\n",
        "        if len(replay_memory) > BATCH_SIZE:\n",
        "            loss.append(optimize_model())\n",
        "\n",
        "        episode_reward += reward.data\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "    losses.append(np.mean(loss))\n",
        "    all_rewards.append(episode_reward)\n",
        "    # Update the target network\n",
        "    if i_episode % TARGET_UPDATE == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "    \n",
        "    if i_episode % EVAL_FREQ == 0:\n",
        "        env.reset()\n",
        "        last_screen = get_screen()\n",
        "        current_screen = get_screen()\n",
        "        state = current_screen - last_screen\n",
        "        episode_reward = 0\n",
        "        frames_for_gif = []\n",
        "        frames_for_gif.append(current_screen.cpu().permute(1, 2, 0).numpy())\n",
        "        for t in count():\n",
        "            # Select and perform an action\n",
        "            # action = select_action(policy_net, state, num_actions)\n",
        "            action = policy_net(state.unsqueeze(0)).max(1)[1].view(1, 1)\n",
        "            _, reward, done, _ = env.step(action.item())\n",
        "            reward = torch.tensor([reward], device=device)\n",
        "\n",
        "            # Observe new state\n",
        "            last_screen = current_screen\n",
        "            current_screen = get_screen()\n",
        "            # plt.imshow(current_screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "            #         interpolation='none')\n",
        "            # plt.show()\n",
        "\n",
        "            frames_for_gif.append(current_screen.cpu().permute(1, 2, 0).numpy())\n",
        "            next_state = current_screen - last_screen\n",
        "\n",
        "            # Move to the next state\n",
        "            state = next_state\n",
        "\n",
        "            episode_reward += reward.data\n",
        "            if done:\n",
        "                # print (\"Timesteps: \", t + 1)\n",
        "                break\n",
        "\n",
        "        generate_gif(i_episode, frames_for_gif, episode_reward, path = f\"/content/gdrive/My Drive/ModelFiles/\")\n",
        "        if i_episode % 2*EVAL_FREQ == 0:\n",
        "\n",
        "            file_counter += 1\n",
        "            path = f\"/content/gdrive/My Drive/ModelFiles/RLP2CartPole_DDQN1-{file_counter}.pth\"\n",
        "\n",
        "            torch.save({'state_dict': policy_net.state_dict(),\n",
        "                        'all_rewards': all_rewards,\n",
        "                        'loss': losses,\n",
        "                        'optimizer': optimizer.state_dict()}, path)\n",
        "            \n",
        "end_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjnalLoVdQBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "print(f'Epoch: {num_episodes+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "path = f\"/content/gdrive/My Drive/ModelFiles/RLP2CartPole-{num_episodes}-Time-{epoch_mins}m{epoch_secs}s.pth\"\n",
        "\n",
        "torch.save({'state_dict': policy_net.state_dict(),\n",
        "            'all_rewards': all_rewards,\n",
        "            'all_timesteps': all_timesteps,\n",
        "            'optimizer': optimizer.state_dict()}, path)\n",
        "\n",
        "print('Model Saved')\n",
        "env.render()\n",
        "env.close()\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E4Xw3ocah2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = f\"/content/gdrive/My Drive/ModelFiles/\"\n",
        "path += f\"RLP2CartPole-1000-Time-85m34s.pth\"\n",
        "# path += f\"RLP2CartPole-200-Time-15m17s.pth\"\n",
        "model = torch.load(path)\n",
        "policy_net = DQN(env).to(device)\n",
        "# optimizer = optim.RMSprop(policy_net.parameters())\n",
        "# target_net = DQN(env).to(device)\n",
        "policy_net.load_state_dict(model['state_dict'])\n",
        "# optimizer.load_state_dict(model['optimizer'])\n",
        "# target_net.load_state_dict(model['state_dict'])\n",
        "policy_net.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBar_8_kcIUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-HB6iBN4Njy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(model['all_rewards'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSPBExLkKHT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GAMMA = 0.999\n",
        "# EPS_START = 0.05\n",
        "# EPS_END = 0.05\n",
        "# EPS_DECAY = 200\n",
        "num_actions = env.action_space.n\n",
        "rewards = []\n",
        "\n",
        "for i in range(200):\n",
        "  env.reset()\n",
        "  last_screen = get_screen()\n",
        "  current_screen = get_screen()\n",
        "  state = current_screen - last_screen\n",
        "  episode_reward = 0\n",
        "  frames_for_gif = []\n",
        "  for t in count():\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Select and perform an action\n",
        "      # action = select_action(policy_net, state, num_actions)\n",
        "      action = policy_net(state).max(1)[1].view(1, 1)\n",
        "      _, reward, done, _ = env.step(action.item())\n",
        "      reward = torch.tensor([reward], device=device)\n",
        "\n",
        "      # Observe new state\n",
        "      last_screen = current_screen\n",
        "      current_screen = get_screen()\n",
        "      # plt.imshow(current_screen.cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
        "      #         interpolation='none')\n",
        "      # plt.show()\n",
        "\n",
        "      frames_for_gif.append(current_screen.cpu().squeeze(0).permute(1, 2, 0).numpy())\n",
        "      next_state = current_screen - last_screen\n",
        "\n",
        "      # Move to the next state\n",
        "      state = next_state\n",
        "\n",
        "      episode_reward += reward.data\n",
        "      if done:\n",
        "        # print (\"Timesteps: \", t + 1)\n",
        "        break\n",
        "  rewards.append(episode_reward)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glHjSL94Dooj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(rewards)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMsHikdQOD2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generate_gif(10000, frames_for_gif, episode_reward, path = f\"/content/gdrive/My Drive/ModelFiles/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSUtOTCdfdgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window = 1\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward (SMA 10)')\n",
        "plt.plot([np.mean(all_rewards[tr:tr+window]) for tr in range(window, len(all_rewards))])\n",
        "# plt.legend(['Exponential Decay Rate - 0.95'])\n",
        "plt.title('Rewards vs Episodes')\n",
        "print(\"reward\", max(all_rewards))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhQOjap2w_NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(all_timesteps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iLZWTNKxLfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}